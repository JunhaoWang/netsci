{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "from functools import reduce\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "from networkx.algorithms import node_classification\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "    All objects above must be saved using python pickle module.\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"datasets/data-gcn/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"datasets/data-gcn/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "\n",
    "    return adj, features, labels\n",
    "\n",
    "def data_helper(dname):\n",
    "    def sub_helper(G, keyname):\n",
    "        labels = [G.nodes[i][keyname] for i in G.nodes]\n",
    "        sorted_labels_set = sorted(list(set(labels)))\n",
    "        labels_idx = np.array(list(map(lambda x: sorted_labels_set.index(x), labels)))\n",
    "        return G, labels_idx\n",
    "    if dname in ['cora', 'citeseer', 'pubmed']:\n",
    "        adj, features, labels =load_data(dname)\n",
    "        labels_idx = np.argmax(labels, axis=-1)\n",
    "        G=nx.from_scipy_sparse_matrix(adj)\n",
    "        return G, labels_idx\n",
    "    elif dname == 'karate':\n",
    "        G=nx.karate_club_graph()\n",
    "        return sub_helper(G, 'club')\n",
    "    elif dname == 'strike':\n",
    "        G=nx.read_gml(\"datasets/real-classic/strike.gml\")\n",
    "        return sub_helper(G, 'value')\n",
    "    elif dname == 'polbooks':\n",
    "        G=nx.read_gml(\"datasets/real-classic/polbooks.gml\")\n",
    "        return sub_helper(G, 'value')\n",
    "    elif 'lfr' in dname:\n",
    "        mu = .1 * int(dname.split('_')[1])\n",
    "        assert mu <= .9 and mu >= .1\n",
    "        G = nx.algorithms.community.community_generators.LFR_benchmark_graph(\n",
    "            1000, 3, 1.5, mu, average_degree=5, min_degree=None, max_degree=None, \n",
    "            min_community=20, max_community=None, tol=1e-07, max_iters=500, seed=None)\n",
    "        for i in G:\n",
    "            G.nodes[i]['community_tuple'] = tuple(sorted(list( G.nodes[i]['community'])))\n",
    "        return sub_helper(G, 'community_tuple')\n",
    "    \n",
    "def partition2label(G, partition):\n",
    "    c = 0\n",
    "    node2idx = {}\n",
    "    for i in G.nodes:\n",
    "        node2idx[i] = c\n",
    "        c += 1\n",
    "    inferred_labels = np.zeros((reduce(lambda x, y: x + y, list(map(lambda z: len(z), partition)))))\n",
    "    assert len(G) == len(inferred_labels)\n",
    "    for ind, i in enumerate(partition):\n",
    "        for j in i:\n",
    "            inferred_labels[node2idx[j]] = ind\n",
    "    return inferred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graphs = {\n",
    "    'lfr_mu_1': lambda: nx.algorithms.community.community_generators.LFR_benchmark_graph(\n",
    "        1000, 3, 1.5, 0.1, average_degree=5, min_degree=None, max_degree=None, min_community=20, \n",
    "        max_community=None, tol=1e-07, max_iters=500, seed=None),\n",
    "    'lfr_mu_5': lambda: nx.algorithms.community.community_generators.LFR_benchmark_graph(\n",
    "        1000, 3, 1.5, 0.5, average_degree=5, min_degree=None, max_degree=None, min_community=20, \n",
    "        max_community=None, tol=1e-07, max_iters=500, seed=None),\n",
    "    'lfr_mu_9': lambda: nx.algorithms.community.community_generators.LFR_benchmark_graph(\n",
    "        1000, 3, 1.5, 0.9, average_degree=5, min_degree=None, max_degree=None, min_community=20, \n",
    "        max_community=None, tol=1e-07, max_iters=500, seed=None),\n",
    "    'karate': lambda: nx.karate_club_graph(),\n",
    "    'strike': lambda: nx.read_gml(\"datasets/real-classic/strike.gml\"),\n",
    "    'football': lambda: nx.read_gml(\"datasets/real-classic/football.gml\"),\n",
    "    'polbooks': lambda: nx.read_gml(\"datasets/real-classic/polbooks.gml\"),\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== cora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "performance = {}\n",
    "for i in ['cora', 'citeseer', 'pubmed', 'karate', 'strike', 'polbooks', 'lfr_3', 'lfr_6', 'lfr_9']:\n",
    "    performance[i] = {\n",
    "        'label_prop': {\n",
    "            'acc': [],\n",
    "        },\n",
    "        'svd': {\n",
    "            'acc': [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "for dname in tqdm(performance):\n",
    "    print('=====', dname)\n",
    "    G, labels = data_helper(dname)\n",
    "    \n",
    "    \n",
    "    \n",
    "    break\n",
    "    for algo in performance[dname]:\n",
    "        if 'lfr' in dname:\n",
    "            runs = 10\n",
    "        else:\n",
    "            runs = 3\n",
    "        for run in range(runs):\n",
    "            if algo == 'label_prop':\n",
    "                partition =  list(nx.community.label_propagation_communities(G))\n",
    "                labels_inf = partition2label(G, partition)\n",
    "            elif algo == 'svd':\n",
    "                aja = nx.to_scipy_sparse_matrix(G)\n",
    "                emb = TruncatedSVD(n_components=min(100, aja.shape[0] // 3)).fit_transform(aja)\n",
    "                clusterer = hdbscan.HDBSCAN()\n",
    "                clusterer.fit(emb)\n",
    "                labels_inf = clusterer.labels_\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "            for metric in performance[dname][algo]:\n",
    "                if metric == 'nmi':\n",
    "                    performance[dname][algo][metric].append(normalized_mutual_info_score(labels, labels_inf))\n",
    "                elif metric == 'ari':\n",
    "                    performance[dname][algo][metric].append(adjusted_rand_score(labels, labels_inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.node[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4, ..., 1, 2, 6], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
